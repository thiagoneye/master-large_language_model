{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercício de Self-Attention com PyTorch\n",
        "Neste exercício, você irá codificar o mecanismo de Self-Attention do zero, com sugestões utilizando a biblioteca PyTorch.\n",
        "\n",
        "Conforme visto em sala, o Self-attention é um componente central dos Transformers, as redes neurais que impulsionam os modelos de linguagem modernos como o BERT. Entender o self-attention é fundamental para compreender como esses modelos conseguem processar e interpretar a linguagem.\n",
        "\n",
        "## Contexto\n",
        "Um Transformer processa um texto de entrada, que é dividido em tokens. O Self-attention permite que o modelo determine a relação entre diferentes tokens em uma sequência.\n",
        "\n"
      ],
      "metadata": {
        "id": "WwxUW7Wi0yTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dicas de ferramental\n",
        "\n",
        "Para implementar será necessário saber realizar algumas operações. Como por exemplo inicializar pesos, multiplicação entre matrizes, calcular transposta etc. Vamos utilizar o framework `PyTorch` como sugestão, mas fica a cargo de cada um escolher a tecnologia que sinta mais à vontade.\n",
        "\n",
        "\n",
        "## Multiplicação de matrizes\n",
        "\n",
        "Podemos utilizar a função `torch.matmul()`. A função matmul pode ser utilizada para realizar a multiplicação de matrizes entre **q** e **k**. O resultado esperado, é uma matriz de pontuações que indica a similaridade de cada query com cada key. Uma pontuação alta significa que um token é muito similar a outro.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Amx776yy-SpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo de multiplicação de matrizes\n",
        "import torch\n",
        "\n",
        "# Define as matrizes\n",
        "matriz_A = torch.tensor([[1, 2], [3, 4]])\n",
        "matriz_B = torch.tensor([[5, 6], [7, 8]])\n",
        "\n",
        "# Multiplica as matrizes\n",
        "torch.matmul(matriz_A, matriz_B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vok-6UctTwX",
        "outputId": "3286bf1a-a307-4938-e7bd-aff52fcca3e7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[19, 22],\n",
              "        [43, 50]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transposta de uma matriz\n",
        "\n",
        "Podemos utilizar o método `.transpose()` para calcular a transposta, conforme exemplo abaixo a seguir. A função do matriz.transpose(0, 1) inverte a primeira dimensão (índice 0, que são as linhas) com a segunda dimensão (índice 1, que são as colunas), resultando na matriz transposta.\n"
      ],
      "metadata": {
        "id": "wgG9dcdotUHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a matriz\n",
        "matriz = torch.tensor([[1, 2, 3],\n",
        "                       [4, 5, 6]])\n",
        "\n",
        "# Transpõe a matriz\n",
        "matriz.transpose(0, 1)"
      ],
      "metadata": {
        "id": "1Dh15LaMABp0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "219adc09-a154-416a-a580-45913a56f711"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 4],\n",
              "        [2, 5],\n",
              "        [3, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Divisão de matrizes\n",
        "\n",
        "Para efetuar o escalonamento das similaridades, evitando que valores muito grandes dominem o processo de `softmax`, será necessário efetuar uma divisão por um valor escalar calculado, conforme a fórmula vista na aula. Segue exemplo de como fazer com PyTorch abaixo:"
      ],
      "metadata": {
        "id": "EhzNjMOJt9il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a matriz\n",
        "matriz = torch.tensor([[10, 20], [30, 40]])\n",
        "escalar = 2\n",
        "\n",
        "# Divide a matriz por um escalar\n",
        "matriz_dividida = matriz / escalar\n",
        "\n",
        "matriz_dividida"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0lTaTISt9FW",
        "outputId": "efdbd170-e4c6-4437-8b4e-640b3ab9209e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 5., 10.],\n",
              "        [15., 20.]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aplicação de Softmax\n",
        "\n",
        "Após o escalonamento, será necessário transformar em porcentagens de atenção as pontuações de similaridade obtidas. Para isso pode se utilizar a função `softmax`, que converte uma lista de números em uma distribuição de probabilidade. Ela garante que todos os valores somem 1. Cada valor na matriz de percentuais calculada representa o peso de atenção, ou seja, a porcentagem de \"importância\" que um token deve dar a todos os outros tokens da sequência.\n",
        "\n",
        "O parâmetro `dim` diz ao PyTorch se essa conversão deve ser feita linha por linha, coluna por coluna ou ao longo de outra dimensão específica. Para nosso caso deve utilizar a dimensão da coluna.\n",
        "\n",
        "```python\n",
        "perc_atencao = torch.nn.functional.softmax(s_similiridades, dim=self.col_dim)\n",
        "```"
      ],
      "metadata": {
        "id": "AwR0wrRO4i-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "# Tarefas do Exercício\n",
        "## 1. Codifique uma Classe Básica de Self-Attention\n",
        "Sua primeira tarefa é completar a classe SelfAttention. Esta classe receberá as codificações de tokens como entrada e executará os cálculos de self-attention para produzir as pontuações de atenção. O método __init__ já está fornecido, mas você precisará preencher o método forward. Levando em consideração a equação vista:\n",
        "\n",
        "\n",
        "\n",
        "$$Attention(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "\n",
        "\n",
        "**Instruções**:\n",
        "\n",
        "* Você trabalhará no método `forward` da classe `SelfAttention`.\n",
        "\n",
        "* Crie os vetores de Queries, Keys e Values: Use as matrizes de pesos fornecidas (self.W_q, self.W_k e self.W_v) para transformar as token_encodings de entrada nos vetores de query (q), key (k) e value (v).\n",
        "\n",
        "* Calcule as Pontuações de Similaridade: Compute as pontuações de similaridade realizando a multiplicação da matriz de queries (q) pela transposta das keys (k\n",
        "T\n",
        " ).\n",
        "\n",
        "* Dimensione as Pontuações: Dimensione as pontuações de similaridade dividindo-as pela raiz quadrada do número de colunas nas keys.\n",
        "\n",
        "* Aplique o Softmax: Aplique a função softmax às pontuações dimensionadas para obter as porcentagens de atenção. Lembre-se de especificar a dimensão sobre a qual o softmax deve ser aplicado.\n",
        "\n",
        "* Calcule as Pontuações Finais de Atenção: Multiplique as porcentagens de atenção pelos values (v) para obter as pontuações finais de atenção.\n",
        "\n",
        "**Observações:**\n",
        "\n",
        "-  Por motivos didáticos, não utilizaremos bias, nem faremos o treinamento dos pesos, nem trabalharemos com geração de embeddings. Portanto basta inicializar definindo um seed, conforme código de exemplo, para que possamos verificar se os cálculos estão sendo feitos corretamente com os valores de embeddings sugeridos.\n"
      ],
      "metadata": {
        "id": "RwAGv2nZ1CJo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HcIOibDO0xdD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Uma classe básica de Self-Attention.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model=2,\n",
        "                 row_dim=0,\n",
        "                 col_dim=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Inicializa os Pesos (W) para queries, keys e values\n",
        "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "\n",
        "        self.row_dim = row_dim\n",
        "        self.col_dim = col_dim\n",
        "\n",
        "    def forward(self, token_encodings):\n",
        "        \"\"\"\n",
        "        Executa o cálculo de self-attention.\n",
        "\n",
        "        Args:\n",
        "            token_encodings (torch.Tensor): Um tensor de entrada com as codificações dos tokens.\n",
        "        Returns:\n",
        "            torch.Tensor: O tensor com as pontuações de atenção.\n",
        "        \"\"\"\n",
        "        # Criação de queries, keys e values\n",
        "        q = self.W_q(token_encodings)\n",
        "        k = self.W_k(token_encodings)\n",
        "        v = self.W_v(token_encodings)\n",
        "\n",
        "        # Computa as pontuações de similaridade\n",
        "        sims = torch.matmul(q, k.transpose(-2, -1))\n",
        "\n",
        "        # Dimensiona as similaridades\n",
        "        d_k = k.size(-1)\n",
        "        scaled_sims = sims / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "\n",
        "        # Aplica softmax para obter as porcentagens de atenção\n",
        "        attention_percents = F.softmax(scaled_sims, dim=-1)\n",
        "\n",
        "        # Calcula as pontuações finais de atenção\n",
        "        attention_scores = torch.matmul(attention_percents, v)\n",
        "\n",
        "        return attention_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## 2. Teste sua Classe de Self-Attention\n",
        "Após completar a classe SelfAttention, você a testará com alguns dados de exemplo.\n",
        "\n",
        "**Instruções:**\n",
        "\n",
        "Execute o código abaixo.\n",
        "\n",
        "A saída do seu código deve corresponder à saída esperada."
      ],
      "metadata": {
        "id": "Nk_piNCA61is"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria uma matriz de codificações de token\n",
        "encodings_matrix = torch.tensor([[1.16, 0.23],\n",
        "                                 [0.57, 1.36],\n",
        "                                 [4.41, -2.16]])\n",
        "\n",
        "# Define a semente para reprodutibilidade\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Cria um objeto de self-attention\n",
        "selfAttention = SelfAttention(d_model=2,\n",
        "                               row_dim=0,\n",
        "                               col_dim=1)\n",
        "\n",
        "# Calcula as pontuações de atenção para as codificações de token\n",
        "selfAttention(encodings_matrix)"
      ],
      "metadata": {
        "id": "XUX2aYQV7EZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4406b5d7-fe09-43b6-fff4-9f2a9c8f6582"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0100, 1.0641],\n",
              "        [0.2040, 0.7057],\n",
              "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Saída Esperada:**\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "tensor([[1.0100, 1.0641],\n",
        "        [0.2040, 0.7057],\n",
        "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "jkGcx2L87IrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# 3. Implemente a Masked Self-Attention\n",
        "Sua tarefa é modificar a classe `SelfAttention` anterior para incluir o comportamento de máscara criar uma nova classe `MaskedSelfAttention`.\n",
        "\n",
        "## **Instruções:**\n",
        "\n",
        "* A partir do código da sua classe `SelfAttention`, adicione uma lógica de mascaramento antes da função softmax.\n",
        "\n",
        "* Você precisará criar uma máscara (mask) que tenha o mesmo formato da matriz de pontuações de similaridade (`sims`). Esta máscara deve conter zeros na diagonal principal e abaixo dela, e `-inf` (número muito grande e negativo) acima dela.\n",
        "\n",
        "* Use a função `torch.triu()` para criar a parte superior da matriz de forma triangular.\n",
        "\n",
        "* Adicione a máscara à matriz de pontuações de similaridade (`sims`). O valor `-inf` garantirá que o softmax transforme a pontuação de atenção em zero, ignorando os tokens futuros.\n",
        "\n",
        "* Mantenha o restante da sua implementação de self-attention, como a escalabilidade e a multiplicação com a matriz de valores, intacto.\n",
        "\n",
        "* Lembre-se que o \"mascaramento\" acontece ao fazer a adição da matriz de máscara, conforme a fórmula derivada a seguir:\n",
        "\n",
        "\n",
        "$$\\text{MaskedAttention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T + M}{\\sqrt{d_k}}\\right)V$$\n"
      ],
      "metadata": {
        "id": "CHPer5Wzupya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Uma classe básica de Masked Self-Attention.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model=2,\n",
        "                 row_dim=0,\n",
        "                 col_dim=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Inicializa os Pesos (W) para queries, keys e values\n",
        "        self.W_q = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "        self.W_k = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "        self.W_v = nn.Linear(in_features=d_model, out_features=d_model, bias=False)\n",
        "\n",
        "        self.row_dim = row_dim\n",
        "        self.col_dim = col_dim\n",
        "\n",
        "    def forward(self, token_encodings):\n",
        "        \"\"\"\n",
        "        Executa o cálculo de Masked Self-Attention.\n",
        "\n",
        "        Args:\n",
        "            token_encodings (torch.Tensor): Um tensor de entrada com as codificações dos tokens.\n",
        "        Returns:\n",
        "            torch.Tensor: Um tensor com as pontuações de atenção mascaradas.\n",
        "        \"\"\"\n",
        "        # Criação de queries, keys e values\n",
        "        q = self.W_q(token_encodings)\n",
        "        k = self.W_k(token_encodings)\n",
        "        v = self.W_v(token_encodings)\n",
        "\n",
        "        # Computa as pontuações de similaridade\n",
        "        sims = torch.matmul(q, k.transpose(-2, -1))\n",
        "\n",
        "        # Pega o tamanho da sequência a partir da matriz de similaridades\n",
        "        seq_len = sims.size(-1)\n",
        "\n",
        "        # Cria uma máscara triangular superior\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len, device=token_encodings.device), diagonal=1)\n",
        "\n",
        "        # Cria a matriz M, substituindo os 1s por -infinito\n",
        "        M = mask.masked_fill(mask == 1, float('-inf'))\n",
        "\n",
        "        # Adiciona a máscara M às pontuações de similaridade\n",
        "        masked_sims = sims + M\n",
        "\n",
        "        # Dimensiona as similaridades\n",
        "        d_k = k.size(-1)\n",
        "        scaled_sims = masked_sims / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
        "\n",
        "        # Aplica softmax para obter as porcentagens de atenção\n",
        "        attention_percents = F.softmax(scaled_sims, dim=-1)\n",
        "\n",
        "        # Calcula as pontuações finais de atenção\n",
        "        attention_scores = torch.matmul(attention_percents, v)\n",
        "\n",
        "        return attention_scores"
      ],
      "metadata": {
        "id": "_qip6xU277eA"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cria uma matriz de codificações de token\n",
        "encodings_matrix = torch.tensor([[1.16, 0.23],\n",
        "                                 [0.57, 1.36],\n",
        "                                 [4.41, -2.16]])\n",
        "\n",
        "# Define a semente para reprodutibilidade\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Cria um objeto de self-attention\n",
        "selfAttention = MaskedSelfAttention(d_model=2,\n",
        "                               row_dim=0,\n",
        "                               col_dim=1)\n",
        "\n",
        "# Calcula as pontuações de atenção para as codificações de token\n",
        "selfAttention(encodings_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j7tb21TihTq",
        "outputId": "345e07ac-b652-4d59-df60-ba95904de3a4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6038,  0.7434],\n",
              "        [-0.0062,  0.6072],\n",
              "        [ 3.4989,  2.2427]], grad_fn=<MmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}