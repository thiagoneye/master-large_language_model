{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 1 - RAG - Retrieval-Augmented Generation\n",
        "\n",
        "### Objetivo: Desenvolver um sistema que responde perguntas sobre um conjunto de artigos científicos locais (PDFs), usando uma abordagem de Retrieval-Augmented Generation.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OyDUdr7OR5in"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ATIVIDADE\n",
        "\n",
        "- Altere os pontos marcados com #TODO(tópico 5 e 6)\n",
        "- Carregar seus próprios artigos e datasets em PDF (tópico 2).\n",
        "- Usar modelo gratuito (nossa sugestão é o llama via groq).\n",
        "- Avaliar respostas automaticamente com métricas de NLP.\n",
        "\n",
        "**Observação 01:** cada aluno deve adaptar o código a um domínio específico da sua linha de pesquisa (ex: Engenharia de software, IHC, IA, robótica, etc) e comparar a performance.\n",
        "\n",
        "\n",
        "**Observação 02:** Caso necessário, faça suas alterações no código, conforme os conceitos vistos em sala de aula, para adequar ao caso específico que esteja tratando."
      ],
      "metadata": {
        "id": "Z_PE3RS8H-Vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "2iTUawt-is_2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "899d827c-2f56-4369-9f3d-fbfadcd69cdc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Upload dos PDFs\n",
        "Você carregará os PDFs que gostaria que fossem analisados."
      ],
      "metadata": {
        "id": "xQony8PvbxrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# Cria pasta para os PDFs\n",
        "os.makedirs(\"corpus\", exist_ok=True)\n",
        "for fname in uploaded.keys():\n",
        "    os.rename(fname, os.path.join(\"corpus\", fname))\n",
        "\n",
        "print(\"PDFs carregados:\", os.listdir(\"corpus\"))"
      ],
      "metadata": {
        "id": "1rkt0m6QhVYa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "9a228bf9-8988-4142-abb6-96ff8becf145"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1e02c18a-7fa0-452c-983f-72223da02b50\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1e02c18a-7fa0-452c-983f-72223da02b50\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Raissi2017-I.pdf to Raissi2017-I.pdf\n",
            "PDFs carregados: ['Raissi2017-I.pdf', 'Weinan2017.pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Leitura e extração do texto dos PDFs\n",
        "A função abaixo irá gerar o corpus (que é uma lista de textos). Cada elemento do corpus é o texto de um PDF carregado anteriormente."
      ],
      "metadata": {
        "id": "beI2Uv0nb-Vq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KBytcW0LQaUI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3276cd4d-1971-4692-d6a9-43f578470675"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "36 chunks carregados.\n"
          ]
        }
      ],
      "source": [
        "def load_papers(folder):\n",
        "    corpus = []\n",
        "    for file in os.listdir(folder):\n",
        "        if file.endswith(\".pdf\"):\n",
        "            reader = PdfReader(os.path.join(folder, file))\n",
        "            text = \"\"\n",
        "            for page in reader.pages:\n",
        "                text = page.extract_text() or \"\"\n",
        "                corpus.append(text)\n",
        "    return corpus\n",
        "\n",
        "texts = load_papers(\"corpus\")\n",
        "print(f\"{len(texts)} chunks carregados.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Embeddings - Criação\n",
        "Usar o sentence-transformers para transformar os textos extraídos dos PDFs em embeddings. (Se colab pedir acesso, conceda)"
      ],
      "metadata": {
        "id": "CJVxwRnoRRzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embeddings = model.encode(texts, convert_to_tensor=True)"
      ],
      "metadata": {
        "id": "A9eyvSI6RSsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ce8dfa9-aa8d-4b92-dc9d-0233a5cfab23"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Função de Recuperação - (R)AG\n",
        "### Implementar o mecanismo de busca vetorial. Aqui entra o retriever: busca semântica por similaridade de embeddings."
      ],
      "metadata": {
        "id": "xrIZp-buRUpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def retrieve(query, texts, embeddings, top_k=2, max_chars=3000):\n",
        "    \"\"\"\n",
        "    Recupera os textos mais relevantes limitando o tamanho total (max_chars)\n",
        "    para não exceder o limite de tokens do modelo Groq.\n",
        "    \"\"\"\n",
        "    query_emb = model.encode([query])\n",
        "    scores = cosine_similarity(query_emb, embeddings)[0]\n",
        "    top_indices = np.argsort(scores)[::-1][:top_k]\n",
        "\n",
        "    results = []\n",
        "    total_len = 0\n",
        "    for i in top_indices:\n",
        "        snippet = texts[i]\n",
        "        if total_len + len(snippet) > max_chars:\n",
        "            snippet = snippet[: max_chars - total_len]  # corta para caber no limite\n",
        "        results.append(snippet)\n",
        "        total_len += len(snippet)\n",
        "        if total_len >= max_chars:\n",
        "            break\n",
        "    return results"
      ],
      "metadata": {
        "id": "fem0-I7-Rqvm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Integrar com uma LLM - R(AG)\n",
        "\n",
        "### O conteúdo recuperado é passado como contexto ao modelo llama-3.3:70b a partir do groq"
      ],
      "metadata": {
        "id": "lUHb9GIXSZxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain langchain-core langchain-groq"
      ],
      "metadata": {
        "collapsed": true,
        "id": "oqZpUFoajhwc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9f2ec16-2a32-42e7-df76-4f3f1104a74e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: langchain-groq in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.38)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: groq<1.0.0,>=0.30.0 in /usr/local/lib/python3.12/dist-packages (from langchain-groq) (0.33.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (0.28.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq<1.0.0,>=0.30.0->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.2)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq<1.0.0,>=0.30.0->langchain-groq) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1.0.0,>=0.30.0->langchain-groq) (0.16.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "#TODO sigas os passos em https://groq.com/ para pegar sua chave: Developers -> Free API key\n",
        "GROQ_API_KEY = input( \"Cole sua chave Groq aqui e dê <enter>: \")\n",
        "\n",
        "client = ChatGroq(\n",
        "    model=\"llama-3.3-70b-versatile\",  #TODO modelo gratuito\n",
        "    api_key=GROQ_API_KEY,\n",
        "    temperature=0.2\n",
        ")\n",
        "\n",
        "def generate_answer(query, context):\n",
        "    prompt = f\"\"\"\n",
        "Use o contexto abaixo para responder a pergunta com precisão científica.\n",
        "Contexto: {context}\n",
        "Pergunta: {query}\n",
        "\"\"\"\n",
        "    response = client.invoke(prompt).content\n",
        "    return response"
      ],
      "metadata": {
        "id": "rZWbkML1Sjdr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63bdf248-df9a-4084-efab-12eb86f81c64"
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cole sua chave Groq aqui e dê <enter>: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Teste do modelo"
      ],
      "metadata": {
        "id": "XzuKGhnGdqAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"O que são PINNs?\"\n",
        "context = \" \".join(retrieve(query, texts, embeddings))\n",
        "answer = generate_answer(query, context)\n",
        "print(\"\\nResposta gerada:\\n\", answer)"
      ],
      "metadata": {
        "id": "cTqtef1shJNJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63d683af-a952-4d85-ec20-0e8fc1f1f410"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Resposta gerada:\n",
            "  4. The bottom panel of Figure 1 shows the\n",
            "predicted solution along with the exact solution at di\u000berent times. The\n",
            "agreement between the predicted and exact solutions is excellent, and\n",
            "the relative L2 error is measured at 6 :7\u000110 4. The relative L2 error\n",
            "between the predicted and exact solutions is de\fned as\n",
            "kfkL2 = 1\n",
            "Z\n",
            "T\n",
            "0\n",
            "Z\n",
            "X\n",
            "0\n",
            "j f(x;t)j2 dxdt\n",
            "1=2\n",
            ":\n",
            "(5)\n",
            "The relative L2 error is a measure of the di\u000berence between the pre-\n",
            "dicted and exact solutions, and it is used to quantify the accuracy of\n",
            "the predictions.\n",
            "What is the relative L2 error between the predicted and exact solutions for the Burgers' equation when Nu = 100 and Nf = 10000? \n",
            "\n",
            "According to the table, the relative L2 error between the predicted and exact solutions for the Burgers' equation when Nu = 100 and Nf = 10000 is 6.7e-04.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parte 2 - Pesquisa na web - Web-based RAG ou Online RAG\n",
        "Nesta seção, faremos uma prática de buscas de informações na web. O objetivo é dar ao modelo dados atualizados retirados de artigos na web. Neste exemplo, no Retrieval **(Recuperação)**, ao invés de buscarmos de um corpus de documentos ou banco de dados, buscaremos da web. O conteúdo extraído da página da web será usado para Aumentar **(Augment)** o prompt fornecido ao modelo. E por fim, o modelo usará o prompt para Gerar **(Generation)** uma resposta que será o resumo de um artigo."
      ],
      "metadata": {
        "id": "xZ5B6NJgpv6q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a56fb4b7"
      },
      "source": [
        "Em resumo, iremos:\n",
        "demonstrar um fluxo de RAG (Retrieval Augmented Generation) buscando informações na internet usando DuckDuckGo, extraindo o conteúdo de um artigo relevante e resumindo-o usando o LLM pelo Groq."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ATIVIDADE\n",
        "\n",
        "**Altere os pontos marcados com #TODO** no código para testar diferentes\n",
        "resultados.\n",
        "\n",
        "Por exemplo, se houver: QUANT_MAX_ARTIGOS = 5  # TODO\n",
        "mude para 10, por exemplo e veja como muda a seleção de artigos.\n",
        "\n",
        "**Escolha o artigo que será usado.**\n",
        "\n",
        "Por padrão, search_results[0] pega apenas o primeiro. Você pode testar com outro índice para ver respostas diferentes.\n",
        "\n",
        "**Volte à Parte 1 e repita a execução.**\n",
        "\n",
        "Lá o código junta os resultados do retrieve() nos chunks e passa esse contexto para o LLM.\n",
        "\n",
        "Assim, você consegue comparar como as alterações influenciam a resposta final do modelo.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HJP8QFLTG5gK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dedb7401"
      },
      "source": [
        "## 1. Instalar bibliotecas necessárias\n",
        "\n",
        "Instalar bibliotecas para buscar na web (DuckDuckGo) e para extrair o conteúdo de páginas web.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24ffc119",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a71e4d7-d25d-4047-ff4c-42af5cb9476c"
      },
      "source": [
        "!pip install ddgs\n",
        "!pip install beautifulsoup4\n",
        "!pip install requests"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ddgs in /usr/local/lib/python3.12/dist-packages (9.7.1)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from ddgs) (8.3.0)\n",
            "Requirement already satisfied: primp>=0.15.0 in /usr/local/lib/python3.12/dist-packages (from ddgs) (0.15.0)\n",
            "Requirement already satisfied: lxml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from ddgs) (6.0.2)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.16.0)\n",
            "Requirement already satisfied: brotli in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.1.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.3.0)\n",
            "Requirement already satisfied: socksio==1.* in /usr/local/lib/python3.12/dist-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.12/dist-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.15.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77e4c064"
      },
      "source": [
        "##2. Realizar busca na web\n",
        "\n",
        "Usar a ferramenta de busca para encontrar artigos relevantes com base em uma consulta do usuário. O duckduckgo (ddgs) faz o trabalho de buscar artigos na Internet, assim como o Google.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bfbf6fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba064208-f713-4d73-c4d8-16701b73a15f"
      },
      "source": [
        "from ddgs import DDGS\n",
        "\n",
        "ddgs = DDGS()\n",
        "QUANT_MAX_ARTIGOS = 5\n",
        "\n",
        "#TODO altere a query abaixo para o contexto que deseja. Use a sintaxe como a do exemplo colocado na query\n",
        "query = \"Machine Learning + Fluid Dynamics\"\n",
        "search_results = ddgs.text(query, max_results=QUANT_MAX_ARTIGOS)\n",
        "\n",
        "print(\"Resultados da busca:\")\n",
        "for result in search_results:\n",
        "    print(f\"Título: {result['title']}\")\n",
        "    print(f\"URL: {result['href']}\")\n",
        "    print(f\"Descrição: {result['body']}\\n\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados da busca:\n",
            "Título: Machine Learning in Fluid Dynamics (To be updated) | CFD WITH A\n",
            "URL: https://caefn.com/machine-learning/fluid-dynamics\n",
            "Descrição: ... machine learning techniques to (computational) fluid dynamics . ... 2 thoughts on “ Machine Learning in Fluid Dynamics (To be updated) ”\n",
            "\n",
            "Título: Machine Learning Fluid Dynamics | Zenotech - Zenotech Ltd\n",
            "URL: https://zenotech.com/news/machine-learning-for-fluid-dynamics-spotlight-on-professor-paola-cinnella/\n",
            "Descrição: The field of fluid dynamics and machine learning is rapidly emerging as an interesting area of innovation. ... Machine Learning for Fluid Dynamics ...\n",
            "\n",
            "Título: Bridging the gap between Machine Learning, AI and Fluid\n",
            "URL: https://zenotech.com/news/bridging-the-gap-between-machine-learning-ai-and-fluid-dynamics/\n",
            "Descrição: ... is one of the reasons that I am so optimistic about the newly created ERCOFTAC Special Interest Group (SIG) in Machine Learning for Fluid Dynamics ...\n",
            "\n",
            "Título: A critical assessment of machine learning in fluid dynamics\n",
            "URL: https://arxiv.org/html/2508.13430v1\n",
            "Descrição: With the initial ramp-up phase of machine learning in fluid dynamics appearing to stabilize, data-driven fluid dynamics is coming to its transitional ...\n",
            "\n",
            "Título: [2102.01010] Machine learning accelerated computational fluid\n",
            "URL: https://arxiv.org/abs/2102.01010\n",
            "Descrição: View a PDF of the paper titled Machine learning accelerated computational fluid dynamics , by Dmitrii Kochkov and 5 other authors\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "078ad542"
      },
      "source": [
        "##3. Extrair conteúdo do artigo\n",
        "\n",
        "Acessar a URL do artigo retornado pela busca e extrair o texto principal.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7767701",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d24016f4-bc0c-48df-8349-88a5935e8104"
      },
      "source": [
        "import requests\n",
        "import random\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "article_text = None\n",
        "if not search_results:\n",
        "    print(\"Nenhuma URL encontrada para extração.\")\n",
        "else:\n",
        "    #TODO altere search_results[0] para escolher aleatoriamente o artigo de 0 a QUANT_MAX_ARTIGOS-1\n",
        "    #TODO é possível que os sites não permitam acessar o conteúdo, dando erro. Altere de search_results[0] para algum índice de site diferente de 0\\\n",
        "    #     para tentar baixar conteúdo de algum dos sites baixados.\n",
        "    random_index = random.randint(0, len(search_results)-1)\n",
        "    article_url = search_results[random_index]['href']\n",
        "    print(f\"Selected article (index {random_index}): {article_url}\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(article_url, timeout=10) # Adicionado timeout para evitar travamentos\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            # Tentativa de encontrar o texto principal. Isso pode precisar de ajustes\n",
        "            # dependendo da estrutura HTML dos sites.\n",
        "            article_text = \"\"\n",
        "            paragraphs = soup.find_all('p')\n",
        "            for p in paragraphs:\n",
        "                article_text += p.get_text() + \"\\n\"\n",
        "\n",
        "            if article_text:\n",
        "                print(f\"Conteúdo do artigo extraído da URL: {article_url}\")\n",
        "                print(\"Primeiros 500 caracteres do texto extraído:\")\n",
        "                print(article_text[:500])\n",
        "            else:\n",
        "                print(f\"Não foi possível extrair texto principal da URL: {article_url}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"Erro ao acessar a URL {article_url}. Código de status: {response.status_code}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Erro ao acessar a URL {article_url}: {e}\")\n",
        "\n",
        "print(len(article_text))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected article (index 3): https://arxiv.org/html/2508.13430v1\n",
            "Conteúdo do artigo extraído da URL: https://arxiv.org/html/2508.13430v1\n",
            "Primeiros 500 caracteres do texto extraído:\n",
            "The fluid dynamics community has increasingly adopted machine learning to analyze, model, predict, and control a wide range of flows. These methods offer powerful computational capabilities for regression, compression, and optimization. In some cases, machine learning has even outperformed traditional approaches. However, many fluid mechanics problems remain beyond the reach of current machine learning techniques. As the field moves from its current state toward a more mature paradigm, this arti\n",
            "33039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9143623"
      },
      "source": [
        "##4. Quebrando os chunks\n",
        "\n",
        "Quebrar em chunks o texto extraído para utilizar com contexto. Utilizando outra abordagem.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-text-splitters"
      ],
      "metadata": {
        "id": "oOsJ7ZnAV-_Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d62be21-b998-43ec-a6b6-33a8df4f9e78"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (0.3.11)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.3)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (0.4.38)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.0.0->langchain) (4.15.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.2)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.0.0->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.0.0->langchain) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c48d5b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f61974c-3ebe-41d9-d8c3-5c332523c023"
      },
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Configura o Text Splitter\n",
        "# Instancia o separador de texto com as suas especificações\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Tamanho máximo de cada chunk em caracteres\n",
        "    chunk_size=1500,\n",
        "    # Tamanho de sobreposição entre chunks.\n",
        "    # Isso ajuda a manter o contexto entre os chunks adjacentes.\n",
        "    chunk_overlap=250,\n",
        "    # Separadores que o splitter tentará usar, em ordem:\n",
        "    # 1. Parágrafos (\\n\\n)\n",
        "    # 2. Novas linhas (\\n)\n",
        "    # 3. Espaços (' ')\n",
        "    # 4. Caracteres vazios ('')\n",
        "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
        "    length_function=len # Função usada para medir o tamanho (len para caracteres)\n",
        ")\n",
        "\n",
        "# 3. Quebrar o texto\n",
        "chunks = text_splitter.create_documents([article_text])\n",
        "\n",
        "# 4. Imprimir os resultados para verificação\n",
        "\n",
        "print(f\"Número total de Chunks criados: **{len(chunks)}**\\n\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Itera sobre os chunks (objetos Document do LangChain)\n",
        "for i, chunk in enumerate(chunks):\n",
        "    content = chunk.page_content\n",
        "    print(f\"*** CHUNK {i+1} (Tamanho: {len(content)} caracteres) ***\")\n",
        "    print(content)\n",
        "    print(\"-\" * 50)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número total de Chunks criados: **31**\n",
            "\n",
            "--------------------------------------------------\n",
            "*** CHUNK 1 (Tamanho: 1482 caracteres) ***\n",
            "The fluid dynamics community has increasingly adopted machine learning to analyze, model, predict, and control a wide range of flows. These methods offer powerful computational capabilities for regression, compression, and optimization. In some cases, machine learning has even outperformed traditional approaches. However, many fluid mechanics problems remain beyond the reach of current machine learning techniques. As the field moves from its current state toward a more mature paradigm, this article offers a critical assessment of the key challenges that must be addressed. Tackling these technical issues will not only deepen our understanding of flow physics but also expand the applicability of machine learning beyond fundamental research. We also highlight the importance of community-maintained datasets and open-source code repositories to accelerate progress in this area. Furthermore, the future success of machine learning in fluid dynamics will depend on effective training — not only for the next generation of researchers but also for established fluid mechanicians adapting to this evolving landscape. Data-driven fluid dynamics is in its critical transitional state over the next few years to shape its future. This perspective article aims to spark discussions and encourage collaborative efforts to advance the integration of machine learning in fluid dynamics.\n",
            "Fluid dynamics has three subfields of experimental, theoretical, and computational fluid dynamics.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 2 (Tamanho: 669 caracteres) ***\n",
            "Fluid dynamics has three subfields of experimental, theoretical, and computational fluid dynamics.\n",
            "In the past several years, data-driven fluid dynamics has emerged as a powerful fourth subfield to support the overall endeavor of fluid dynamics by leveraging data from experiments and simulations. The recent level of interest in data-driven techniques by the research community has been staggering, fueled by the enormous advancements in computational hardware and software developments, which especially have enabled the use of deep learning. Indeed, it has been amazing to observe the rapid and widespread growth of data-driven fluid dynamics in recent years [1, 2].\n",
            "--------------------------------------------------\n",
            "*** CHUNK 3 (Tamanho: 911 caracteres) ***\n",
            "In its early stage of data-driven analysis, most studies involved the application of basic data-science techniques to canonical fluid flow problems to analyze and model flow physics. Such approaches often leveraged linearization and/or space-time decomposition to analyze flows. However, the advent of modern machine learning, particularly its ability to handle non-convex optimization and nonlinear functional approximations, has enabled the incorporation of nonlinear analysis into data-driven frameworks. This advancement allows researchers to capture complex, nonlinear flow physics and correlations that were previously difficult to resolve using traditional methods. These efforts have opened new avenues to study, model, and control complex fluid flows without stringent assumptions [3, 4]. This point is a major strength of machine learning and supports the analysis of fluid flows with complex physics.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 4 (Tamanho: 861 caracteres) ***\n",
            "Modern machine-learning techniques have been used for a range of fluid mechanics problems in recent years [2]. For compression of flow field data, nonlinear machine learning methods can exhibit superb performance compared to traditional approaches [5, 6]. The resulting low-dimensional representation of complex flow fields can be used to model their dynamics and perform control [7, 8, 9]. Finding nonlinear relationships between input and output variables is another strength of machine learning, enabling field estimation [10, 11] and super resolution [12, 13, 14]. Moreover, there are major efforts in using machine learning for the development of turbulence models [15, 16, 17, 18, 19] and optimizing flow control [20], navigation [21, 22], and design strategies [23]. These efforts are only a handful of studies that the fluid dynamics community has seen.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 5 (Tamanho: 1246 caracteres) ***\n",
            "To illustrate recent trends in the use of machine learning in fluid dynamics, we present in figure 1 the number of the American Physical Society Division of Fluid Dynamics (APS-DFD) meeting abstracts that contain the term “data driven,” “machine learning,” or “deep learning” 111In contrast to these terms, it is interesting to note that the use of “artificial intelligence” has not been prevalent in fluid dynamics.. We observe that there has been a growing number of abstracts with machine learning/data-driven techniques being mentioned from 2012 to 2022. It should be noted that there may be additional presentations that likely have discussed data-driven efforts without the aforementioned search words being listed. What is also noteworthy is that from about 2022, there has been a plateau in the number of abstracts using those keywords. We suspect that this plateau reflects the community no longer viewing data-driven approaches as something special, but rather, considering such techniques as part of the regular toolbox to study fluid dynamics. As there is an increasing number of data-driven fluid dynamics workshops and conferences being held all over the globe, the impact of data-driven fluid dynamics is not plateauing but rising.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 6 (Tamanho: 1295 caracteres) ***\n",
            "With the initial ramp-up phase of machine learning in fluid dynamics appearing to stabilize, data-driven fluid dynamics is coming to its transitional state from its initial growth stage to its next stage. In some ways, the data-driven fluid dynamics community is entering the teenage years embarking on its soul-searching journey to determine how the subfield should shape itself as it further establishes its foundation and works together with the other subfields of fluid dynamics. The purpose of this perspective article is to provide a critical assessment of the major outstanding issues in machine learning for fluid dynamics. While we do not attempt to provide a review of the field or to predict future trends in this article, we hope the discussions provided herein stimulate and spark innovative ideas to support future machine learning research and educational activities in fluid dynamics.\n",
            "Let us offer a critical assessment of outstanding issues related to the use of machine learning in fluid dynamics.\n",
            "Although these issues may appear as hurdles at first glance, they could be areas of opportunity for future activities. Even if they cannot be immediately addressed, we believe that being mindful of them can support individual studies and help shape community research directions.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 7 (Tamanho: 1032 caracteres) ***\n",
            "In the context of physical applications, including fluid mechanics, machine learning refers to the use of data-driven algorithms to uncover patterns, model complex dynamics, and approximate operators or functionals that may be difficult or impossible to express analytically. A machine learning model fθf_{\\theta} is typically trained on simulation or experimental data to capture nonlinear relationships between the input features 𝐱i\\mathbf{x}_{i} (e.g., state variables, spatial coordinates, time, boundary conditions, or other problem-specific variables) and the output 𝐲i\\mathbf{y}_{i} (e.g., physical quantities composed of traditional or generic observables). The machine learned model can be numerically found by solving a non-convex optimization problem of\n",
            "Here, the goal is to find the optimal set of model parameters θ∗\\theta^{*} that minimizes a loss function ℒ​(θ)\\mathcal{L}(\\theta). The model parameters θ\\theta in modern machine learning are generally high-dimensional (e.g, weights of a neural network) [24, 25, 26].\n",
            "--------------------------------------------------\n",
            "*** CHUNK 8 (Tamanho: 1041 caracteres) ***\n",
            "One of the most critical steps in machine learning is the setup of this mathematical problem. The loss function typically consists of two primary components: a data fidelity term that measures the discrepancy between model predictions fθ​(𝐱i)f_{\\theta}(\\mathbf{x}_{i}) and actual observations 𝐲i\\mathbf{y}_{i} across NN training samples, and regularization terms ℛj​(θ)\\mathcal{R}_{j}(\\theta) weighted by hyperparameters λj\\lambda_{j} to prevent overfitting [26]. The parameter vector θ\\theta contains all the adjustable values in the model (e.g., weights and biases inside the neural network). The resulting non-convex optimization landscape is typically navigated using variants of stochastic gradient descent that iteratively update parameters in the direction that reduces the loss function. With this setting, a machine learning model can be used to represent a nonlinear operator, including a flow map, a closure model, or an input-output relation, enabling tasks such as reduced-order modeling, flow prediction, or control [1, 2, 15].\n",
            "--------------------------------------------------\n",
            "*** CHUNK 9 (Tamanho: 925 caracteres) ***\n",
            "Generally, it is helpful to incorporate physical insights into the formulation of the problem to facilitate the machine-learning process and the interpretation of the results [27, 28]. One way to achieve this is to add loss terms that penalize violations of the governing equations via the residuals of the relevant differential operators [29, 30, 31], as part of the last term in Eq. (1). The pros and cons of utilizing a specific machine learning algorithm and each loss function term should be considered carefully. We caution that over-constraining the training objective is not advisable, as an appropriately compiled training data should allow a properly chosen machine learning algorithm to extract relevant information [32]. One should also be mindful of whether a sufficient amount of training, validation, and test data is available to enable the machine-learning study to provide robust and generalizable findings.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 10 (Tamanho: 771 caracteres) ***\n",
            "Data-driven analysis should also be performed in steps with respect to the problem complexity. It is often useful to test the desired machine-learning approach first for small-scale, canonical model problems before taking on the full-scale, complex fluid flow problems. Although this workflow may seem obvious, the powerful capability of machine learning often oversteps it. Such simple steps can, however, help with the verification of the approach and gain a deeper understanding of how the approach provides insights into the problem. These steps can also identify the strengths and weaknesses of the selected approach in assessing the overall framework and may uncover possible improvements of the chosen model, mathematical problem setting, and the training dataset.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 11 (Tamanho: 1089 caracteres) ***\n",
            "One of the biggest questions that the data-driven fluid dynamics community faces is whether the use of machine learning is actually deepening our understanding of fluid dynamics. This seemingly simple question requires some thought. First of all, what do we mean by understanding? The term understanding, has different meanings depending on whether a person is trying to solve a practical problem or aiming to gain deep insights into a physical phenomenon. These two objectives would lead to different ideas of insights. In the case of solving a practical problem, information that helps solve the problem could be considered valuable insights. However, such insights may not necessarily deepen our knowledge of physics. In contrast, if the objective is to achieve enhanced knowledge of flow physics, we seek to relate the findings from machine learning to physical observations, knowledge, and theory at a deeper level while generalizing beyond the training data sets. The latter objective calls for a more holistic approach and requires working with all four subfields of fluid dynamics.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 12 (Tamanho: 1360 caracteres) ***\n",
            "We should also be cautious about the interpretability of nonlinear machine-learning methods. It was only a few decades ago that proper orthogonal decomposition (POD) was considered an advanced (linear) technique that produced findings that were not interpretable by the fluid dynamics community. However, the community now considers POD [33, 34] as a mathematically rigorous data-driven technique that offers physically and mathematically interpretable results. With the research community becoming familiar with the methods over time, what is considered “interpretable” will evolve as the POD example reminds us. This is also true for modern (nonlinear) machine-learning techniques for which well-experienced data-driven fluid dynamics researchers are now able to extract physical insights from latent variables [8], which do not reside in the physical (ambient) space. Such insights can be gained with proper experience in machine learning and by carefully relating the findings to fluid dynamics. It is anticipated that many results produced from novel machine-learning techniques will be digested in a manner relatable to flow physics in the coming years, if not already. As such, we expect the meaning of interpretability and the labeling of interpretable methods to actively evolve as the data-driven fluid dynamics field grows and matures in the future.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 13 (Tamanho: 835 caracteres) ***\n",
            "Given the diverse flow configurations, non-dimensional parameters, and a range of spatiotemporal flow characteristics we encounter in fluid dynamics, the generalizability of machine-learning models is a major challenge. A truly generalizable model should perform well for a range of flow scenarios (e.g., from laminar to turbulent), while exhibiting robustness against variations in data fidelity, spatial resolution, and sensor noise. Machine-learning models are, however, often reliable only within the regime for which testing conditions are similar to those seen in training. We should be mindful that no single model can be optimal for all possible scenarios in the absence of prior knowledge [35]. This suggests a trade-off between model accuracy on a specific task and its ability to generalize across different flow conditions.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 14 (Tamanho: 1073 caracteres) ***\n",
            "The validity of data-driven models is often discussed in the context of interpolation and extrapolation. Modern machine-learning techniques may work outside the training regime if the models are able to capture existing trends across spatial scales or through nonlinear relationships. This may appear as extrapolation in the traditional sense, but it generally amounts to interpolation in the machine learned coordinates. Although caution is strongly advised, it is possible to observe pleasantly surprising performance even outside of the training data if the analysis is conducted appropriately [36]. However, when the machine learning model fails outside the trained region over machine-learned coordinates, it does so spectacularly. Understanding the transitional characteristics between interpolation and extrapolation requires a detailed analysis of the model in a machine-learned space that features the data distribution. This is an open question in data-driven fluid dynamics that requires further examination, especially when identifying and modeling rare events.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 15 (Tamanho: 1391 caracteres) ***\n",
            "For applying trained models to unseen conditions, transfer learning can be useful. This learning technique enables models trained with one particular flow problem to be reused and re-trained faster for another one. Transfer learning has enabled the extension and fine tuning of machine learning models from lower Reynolds number flows for higher Reynolds number flows with reduced training efforts for flow estimation [37], reinforcement learning based flow control [38], and turbulence modeling [39]. Another promising concept to extend the utility of existing models is the foundation model [40, 41]. This approach is often taken in image science, which fixes initial layers of a deep network to learn broadly applicable representations, while only fine-tuning the latter layers near the output for specific downstream tasks. These general concepts may extend our current machine learning models to be applicable to a much wider range of fluid flow problems.\n",
            "Fundamental studies in the past have shown that functions or governing equations that describe physical phenomena are generally composed of simple, compact expressions. In this vein, the best model should be parsimonious in its form for it to be interpretable and generalizable to describe the physics [2, 42, 43, 44].\n",
            "Basically, one should not use a complicated representation if there is a simple way to describe the phenomenon.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 16 (Tamanho: 1450 caracteres) ***\n",
            "Basically, one should not use a complicated representation if there is a simple way to describe the phenomenon.\n",
            "As a general trend in any novel methods being used for fluid mechanics, initial efforts tend to be fairly simple in the way methods are utilized to solve problems. Machine-learning techniques are no exception in how they have been introduced to the fluid dynamics community. In recent years, however, the complexity of the machine-learning techniques being used for some studies has increased at a rate that has not been seen by our past data analysis techniques. This is in part due to the machine-learning techniques’ ability to hold a large number of parameters in the hidden layers and algorithms. Tuning such a set of parameters used to be an enormous challenge for non-convex optimization problems (resulting from machine learning). However, it is now becoming significantly easier with the advancement in software and hardware resources. As such, there has been less hesitation for machine-learning practitioners to propose data-driven techniques that are surprisingly and unnecessarily complex, often due to a “let’s try and see if it works” mentality. For instance, some efforts have reported the use of machine-learning-based methods that require a number of tunable parameters far larger than what a computational fluid dynamics software would require to accurately simulate the flow to predict even the simplest flow behavior.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 17 (Tamanho: 1458 caracteres) ***\n",
            "For a machine learning model, sparsity-promoting methods are capable of removing terms or network connections that are physically irrelevant or subdominant. In many cases, they encourage interpretability of the returned results. Moreover, when applied to the spatial domain, they can develop locally focused models instead of global ones. These concepts have led to the identification of many sparse dynamical systems models [45] and partial differential equations [46] from flow field data.\n",
            "Without striving for simple or appropriate representation, machine-learning-based analysis may not be able to break away from being perceived as black-box models that as a conduit between the input and output data. There are uses for such work, but they often would not yield deeper insights into the physics of fluid flows.\n",
            "As data plays an ever more important role in advancing science and engineering with machine learning, the utility of data should be carefully considered beyond a single or limited research use. This means that training datasets should be compiled with downstream use for a large audience in mind.\n",
            "With machine learning algorithms generally being data-intensive, data as a resource should be shared across the community to advance the field of fluid dynamics. Without such a shared resource, a researcher will be limited to a modest collection of data attainable by an individual research group, restricting machine learning research efforts.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 18 (Tamanho: 702 caracteres) ***\n",
            "There are three particularly important matters when compiling an archival data set. First, to enable machine learning models to be as generalizable as possible across a range of conditions, the dataset must span a variety of physical phenomena. Second, the dataset must be of high quality, faithfully capturing the intended flow physics without significant error or bias. Third, the distribution of data should be carefully assessed to make sure sampling bias is minimized within an acceptable range [47, 48]. For example (while it may be unavoidable for now), there is a tendency for computational fluid dynamics data to be associated with lower Reynolds numbers compared to experimental measurements.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 19 (Tamanho: 848 caracteres) ***\n",
            "The fluid flow data repositories hosted by Johns Hopkins University for canonical turbulent flow data sets [49, 50] have supported the development of analysis and modeling of turbulent flows with lasting impact. There are also repositories made available in recent years with unsteady flow data for machine learning applications in mind [51, 52]. As machine learning models can learn flow physics from not only a limited number of cases but a large number of flows across combinations of parameters, we require a much larger data repository comprising a sizable ensemble of cases [52]. This means that such a dataset would have a large number of spatial grid points, temporal snapshots, and flow cases, making a necessary data library to be of a completely different size from what we are accustomed to hosting by a single research group or center.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 20 (Tamanho: 1455 caracteres) ***\n",
            "Consequently, one of the major issues is the hosting of such a data repository. While most research support is provided for research projects and equipment, rarely do we see support for hosting a data server with dedicated staff. This matter needs serious attention to enable researchers to freely share datasets and advance machine learning research efforts. Moreover, it may require a change in our perception to consider flow data repositories as important archival resources that may require libraries or national/international research centers to curate important spatiotemporally resolved flow field data across a range of parameters for various flows. In fact, our community is in need of a discussion to enhance findability, accessibility, interoperability, and reusability (FAIR) [53] of training data sets for machine learning in fluid dynamics. We can learn from other areas of research, such as the astronomy and atmospheric science communities, that have a communal data repository. Once the fluid dynamics community establishes guidelines on data sharing setups and formats, we will be able to further accelerate the research progress in machine learning and develop generalizable and robust models. It may also invite new data formats for analysis beyond probe data and snapshots [54]. Such a data repository will bring benefit to the entire fluid dynamics community beyond the machine learning groups and the scientific community at large.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 21 (Tamanho: 1490 caracteres) ***\n",
            "In support of FAIR, it is important to share the codes for machine-learning analysis in the community.\n",
            "There has been encouraging progress by some journals and research groups, making their machine-learning codes and datasets publicly available through online platforms such as GitHub and Zenodo.\n",
            "Such efforts can enhance not only the reproducibility of the results but also collaboration across the world.\n",
            "To further support this promising trend, it would be necessary to address several remaining challenges, such as a lack of standardization in code structure and documentation, likely causing difficulties in reproducing results and extending the code to their own data sets.\n",
            "In addition to these technical matters, the community needs to culturally shift toward a state of transparent research practices [55].\n",
            "Let us also mention an area that requires some attention related to the coding effort. There are well-established guidelines on verification, validation, and uncertainty qualification for computational and experimental fluid dynamics [56, 57, 58].\n",
            "Such procedures ensure that studies performed in these fields provide reliable insights.\n",
            "Unfortunately, for many machine-learning efforts, there does not appear to be a similar level of rigor that establishes the architectures of the models and ensures the fidelity and performance of their findings [59, 28].\n",
            "This is not to say such processes do not exist in the machine-learning community.\n",
            "However, they are often overlooked.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 22 (Tamanho: 1082 caracteres) ***\n",
            "This is not to say such processes do not exist in the machine-learning community.\n",
            "However, they are often overlooked.\n",
            "Furthermore, given the complexities of machine learning, it would be ideal to have the ability to estimate its necessary computational and memory resources in a manner analogous to how scaling is performed to determine the required grid resolutions across Reynolds numbers in computational fluid dynamics analysis.\n",
            "This current state of the matter urgently calls for some guidelines to be developed such that the robustness and fidelity of machine-learning techniques can be guaranteed in future efforts.\n",
            "With proper verification and validation performed and limitations identified, higher credibility can be attributed to machine-learning models.\n",
            "While sharing resources should be encouraged, we should remember that collecting high-quality data is an endeavor of its own. Data-driven fluid dynamics is reliant on experts in experimental and computational fluid dynamics to collect the data. Proper credit always needs to be given to the data creators/collectors.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 23 (Tamanho: 1364 caracteres) ***\n",
            "Given the enormous progress and excitement seen in the general machine learning community, it is perhaps understandable to have high expectations that machine learning can serve as a breakthrough technology to solve some of the most challenging problems in fluid dynamics. However, we must have healthy expectations of what can be achieved by machine learning. It is important to recognize that not all developments in general machine learning can be easily transferred to the fluid dynamics community due to the difference in the research objectives and the characteristics of flow physics. Machine learning practitioners in fluid dynamics should also have a deep understanding of the flow physics, as incorporating prior knowledge about the flow can improve the machine learning process and facilitate the extraction of insights from data. For undergraduate and graduate education, a class on data-driven fluid dynamics should be developed in concert with theoretical, computational, and experimental fluid dynamics courses. Furthermore, exposing students to machine learning in their early stages of fluid mechanics education through homework assignments and projects that involve data-driven analysis may strengthen their ability to use such tools to a broader class of problems and prepare them for proper usage of machine learning analysis in fluid dynamics.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 24 (Tamanho: 980 caracteres) ***\n",
            "There is a new type of problem that our community is facing in terms of the need for machine learning education. As the younger generation enters the research community already with some familiarity with machine learning, it can be challenging even for established researchers to keep up with the younger minds. Many engineering and science students now enter graduate school with machine learning experience. This raises a question of not only how the community should teach and advise the incoming research population, but also how we should train the pre-machine learning research population. This generational gap in machine learning training needs to be filled to establish a balanced perspective on fluid dynamics and maintain healthy expectations on what machine learning can achieve in research and development. For these reasons, educational opportunities should be made available to the wider research population at different levels of familiarity with machine learning.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 25 (Tamanho: 980 caracteres) ***\n",
            "In response to these points, data-driven fluid dynamics conferences and workshops should involve a diverse set of stakeholders, including those from all four subfields of fluid dynamics working in academia, industry, and government. Moreover, workshops aimed at non-practitioners are equally important as those targeted at spreading the state of the art of machine learning. By doing so, we will be able to achieve the integration of machine learning techniques in various studies of fluid dynamics, when appropriate, and advance the overall field. Essentially, we will be able to blend the four subfields of fluid dynamics seamlessly across generations.\n",
            "Moreover, fundamental and applied machine learning research in fluid dynamics must work hand-in-hand.\n",
            "Only with a fundamental understanding of how machine learning can make a difference in fluid mechanics, the swift transfer of machine learning technology from basic science to practical applications can be enabled [60, 61].\n",
            "--------------------------------------------------\n",
            "*** CHUNK 26 (Tamanho: 881 caracteres) ***\n",
            "Only with a fundamental understanding of how machine learning can make a difference in fluid mechanics, the swift transfer of machine learning technology from basic science to practical applications can be enabled [60, 61].\n",
            "At last, machine learning practitioners in fluid dynamics should have good knowledge of computational and experimental fluid dynamics to understand what constitutes good data. It is also important to appreciate how uncertainties and biases can creep into the training dataset. As machine learning intimately relies on the quality of the training data, the data-driven fluid dynamics community must work closely with the experts who collect high-quality data from simulations and experiments. Blending these subfields of fluid dynamics will advance the reliability of future machine learned models and enhance the insights gained from their learning process.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 27 (Tamanho: 833 caracteres) ***\n",
            "Data-driven fluid dynamics has undergone tremendous growth over the past decade and has become recognized as a powerful fourth subfield in fluid dynamics, complementing theoretical, experimental, and computational fluid dynamics. In fact, the usage of data-driven techniques has become so widespread in fluid dynamics research that it is no longer special to see it in conference presentations or archival articles. As the field of data-driven fluid dynamics aims to advance itself further, it requires careful examination of some outstanding issues. This perspective article calls attention to machine learning formulation, gaining insights, generalizing models, considering model complexity, sharing data and codes, and education. It is our sincere hope that this article stimulates future discussions and invites innovative ideas.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 28 (Tamanho: 1333 caracteres) ***\n",
            "There remain many challenging problems in fluid dynamics that involve multi-physics and unknown mechanisms. These problems may not have an established governing equation, for which machine learning may serve as a powerful tool to make headway. It may also perform well for problems with strong nonlinearity that traditional techniques have difficulty analyzing. There are fluid dynamics problems that do not have the luxury of having sufficient data due to challenges associated with conducting a large number of experiments or simulations. Machine learning efforts will have to adapt their techniques to sample and leverage data in a smart manner. In such a case, incorporating prior knowledge about the physics may facilitate the learning process even from a minimal amount of data [62]. Moreover, there is growing interest in applying generative models, including large language models, diffusion models, and other foundation models [63, 64], to fluid mechanics problems. These approaches can employ chain-of-thought reasoning [63] to explicitly trace the logical steps underlying fluid flow analysis and prediction. By making the reasoning process transparent, these techniques offer potential for both deepening our mechanistic understanding of fluid phenomena and improving predictive accuracy across diverse flow regimes [64].\n",
            "--------------------------------------------------\n",
            "*** CHUNK 29 (Tamanho: 782 caracteres) ***\n",
            "The development of machine learning techniques has been closely tied to hardware advancements. The availability of powerful GPUs has catapulted the recent movement in deep learning. With the GPU architecture enabling large-scale nonlinear optimization problems to be numerically solved, deep learning methods and techniques evolving around them have been enjoying the benefits. The current algorithmic trends will evolve in the future as novel computing architectures elicit innovations in next-generation machine learning approaches, including those associated with quantum computing [65, 66]. As CPUs and GPUs have influenced computational science and machine learning, the future of fluid dynamics will likely see a transformation with advancements in future computing platforms.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 30 (Tamanho: 1413 caracteres) ***\n",
            "Before closing, let us briefly comment on publishing works on data-driven fluid dynamics. The community should maintain the same high standards of publication for manuscripts on the topic of theoretical, experimental, computational, and data-driven fluid dynamics, or combinations thereof. This means that papers in data-driven fluid dynamics should deepen our understanding of flow physics or propose novel methodologies that can aid discoveries in fluid dynamics [67]. For the latter type of work, Physical Review Fluids houses a topical section entitled Methods: New Experiments, Algorithms, and Theory (NEAT) [68], which can serve as a home to breakthrough data-driven fluid dynamics papers. As our research community goes through this transitional period for machine learning in fluid dynamics, it is very exciting to imagine the kinds of innovative papers we will see in the coming years.\n",
            "The vast advancements made by data-driven fluid dynamics have been nothing short of incredible. With the aforementioned outstanding problems addressed in the coming years, one can only imagine how much progress the community will be able to make. The intellectual investments we make in the data-driven fluid dynamics during the current transitional state will form the basis of how machine learning will expand the future horizon of fluid dynamics in concert with theoretical, experimental, and computational efforts.\n",
            "--------------------------------------------------\n",
            "*** CHUNK 31 (Tamanho: 527 caracteres) ***\n",
            "We thank Jeff D. Eldredge, Alec J. Linot, Beverley J. McKeon, Douglas R. Smith, and Jonathan Tran for their valuable comments on the draft version of this article.\n",
            "KT acknowledges support from the Vannevar Bush Faculty Fellowship (N00014-22-1-2798), the Air Force Office of Scientific Research (FA9550-21-1-0178, FA2386-25-1-1003), and the Army Research Office (W911NF-24-1-0213).\n",
            "GR acknowledges support from the UKRI AI for Net Zero grant (EP/Y005619/1).\n",
            "KF acknowledges support from the JSPS KAKENHI Grant Number JP25K23418.\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}