{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "497db21d-4e08-42a7-9258-90fecced929f",
      "metadata": {
        "id": "497db21d-4e08-42a7-9258-90fecced929f"
      },
      "source": [
        "# Exercício 1 - Token Embeddings (Operações e contextualização)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TgejuBG0gFF-",
      "metadata": {
        "id": "TgejuBG0gFF-"
      },
      "source": [
        "Conforme visto em sala de aula, existem várias técnicas de tokenização: word token, subword token, character token, byte token, dentre outras que podem obter resultados mistos. Esse exercício abordará embeddings para word tokens, ou word embeddings. Nesse contexto, siga as instruções:\n",
        "\n",
        "**a)** escolha no mínimo 2 grupos de palavras relacionadas, cada grupo contendo no mínimo 3 palavras.\n",
        "Ex.:\n",
        "\n",
        "*   Palavras relacionadas a paisagem\n",
        "    *   ceu\n",
        "    *   montanha\n",
        "    *   rio\n",
        "*   Palavras relacionadas a estudo\n",
        "    *   caderno\n",
        "    *   escola\n",
        "    *   livro\n",
        "\n",
        "**b)** Em seguida, obtenha os embeddings das palavras.\n",
        "\n",
        "**c)** Utilize algum método de redução de dimensionalidade para reduzir os vetores dos embeddings para apenas 2 dimensões.\n",
        "\n",
        "**d)** Faça o \"plot\" em um plano bidimensional com os valores obtidos, comparando os resultados referentes aos diferentes modelos.\n",
        "\n",
        "**e)** Execute operações de soma e subtração com os vetores e observe os resultados obtidos.\n",
        "\n",
        "**f)** Para comparar outras formas de gerar embeddings, utilize o modelo BERT de forma a gerar word embeddings a partir de frases e veja o resultado obtido."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc50c2b6-0745-4952-83c8-170275da0e2d",
      "metadata": {
        "id": "bc50c2b6-0745-4952-83c8-170275da0e2d"
      },
      "source": [
        "## Sugestão: Utilize bibliotecas para ter acesso a modelos pré-treinados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d993bfb-80c1-4817-8622-a065d71da414",
      "metadata": {
        "id": "2d993bfb-80c1-4817-8622-a065d71da414"
      },
      "source": [
        "<p style=\"background-color:#fff6e4; padding:15px; border-width:3px; border-color:#f5ecda; border-style:solid; border-radius:6px\"> ⏳ <b>O código a seguir instala bibliotecas para permitir utilizar os embeddings do GloVe, Word2Vec e BERT</b>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uj9XEaqrmtbJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj9XEaqrmtbJ",
        "outputId": "6ed815bf-4b35-4045-ea53-8b3846d27326"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n"
          ]
        }
      ],
      "source": [
        "#!pip install -r requirements_CE.txt\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zCCjPAZgfVnm",
      "metadata": {
        "id": "zCCjPAZgfVnm"
      },
      "source": [
        "A seguir, exemplo de código utilizando a lib gensim para carregar vetores de modelos pré-treinados como GloVe e Word2vec."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2b7afc1-6894-42e0-a3c9-479372f1da38",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2b7afc1-6894-42e0-a3c9-479372f1da38",
        "outputId": "c43dffb7-425b-4a54-99f7-035a61fb9b8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
          ]
        }
      ],
      "source": [
        "# Suprimindo warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Sugestão de utilização\n",
        "# Importando libs necessárias\n",
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Imprimindo a lista de modelos disponíveis em gensim-data\n",
        "print(list(api.info()['models'].keys()))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WxZD_1Eof54i",
      "metadata": {
        "id": "WxZD_1Eof54i"
      },
      "source": [
        "Os modelos sugeridos GloVe e Word2vec estão disponíveis a partir de 'glove-wiki-gigaword-100' e 'word2vec-google-news-300', respectivamente. Deve demorar alguns minutos dependendo do tamanho do modelo que você escolher."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u8QW27aUf4aC",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8QW27aUf4aC",
        "outputId": "9f942530-8f32-4fbd-9fcf-23434b004664"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        }
      ],
      "source": [
        "# Carregue os vetores dos modelos pré-treinados utilizando o método \"load()\" do objeto api.\n",
        "glove = api.load('fasttext-wiki-news-subwords-300')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n2ivn_0dDM8j",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2ivn_0dDM8j",
        "outputId": "341d789c-b1b3-4af2-a7cc-5f22f4031690"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(300,)\n"
          ]
        }
      ],
      "source": [
        "# Verificar o tamanho do vetor de uma palavra\n",
        "print(glove['casa'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ABpl1JD_Bi9v",
      "metadata": {
        "id": "ABpl1JD_Bi9v"
      },
      "source": [
        "# **EXERCÍCIO**\n",
        "# **a)** Escolha suas palavras, inicialmente em idioma inglês."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b4d461-4905-4a3b-9f49-5373cfa64b7e",
      "metadata": {
        "id": "80b4d461-4905-4a3b-9f49-5373cfa64b7e"
      },
      "outputs": [],
      "source": [
        "# Palavras em inglês\n",
        "words = [\"king\", \"princess\", \"monarch\", \"throne\", \"crown\",\n",
        "         \"mountain\", \"ocean\", \"tv\", \"rainbow\", \"cloud\", \"queen\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ewjGktYyCNqe",
      "metadata": {
        "id": "ewjGktYyCNqe"
      },
      "source": [
        "Agora obtenha os embeddings do modelo carregado anteriormente acessando a chave no vetor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qY-c731LHVLG",
      "metadata": {
        "id": "qY-c731LHVLG"
      },
      "outputs": [],
      "source": [
        "# O modelo funciona como um dicionário, acesse a chave correspondente ao token\n",
        "#Obter embeddings em um dicionário\n",
        "emb_palavras = {word: glove[word] for word in words}\n",
        "\n",
        "# Imprimindo os 5 primeiros valores para cada embedding\n",
        "#print(emb_palavras[:, :5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aDbbD4at4eNw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "collapsed": true,
        "id": "aDbbD4at4eNw",
        "outputId": "5a845074-798e-40b7-cdf5-d9134960480e"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-338005120.py, line 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-338005120.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    print(emb_palavras[,:5])\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "print(words)\n",
        "print(emb_palavras[,:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hw8OM0oaDYC6",
      "metadata": {
        "id": "hw8OM0oaDYC6"
      },
      "source": [
        "Teste os modelos carregandos tentando imprimir algumas palavras em português veja o que acontece. Lembre que o modelo foi treinado sem caracteres acentuados ou case do caractere."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VXWLCQLjDm8k",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXWLCQLjDm8k",
        "outputId": "5586ed41-9412-41b9-de28-857d8110c682"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(300,)\n",
            "[-0.024405   -0.0043346  -0.011769    0.11453     0.023041   -0.09168\n",
            "  0.0094031  -0.14461     0.0035586   0.069339   -0.050251   -0.011979\n",
            " -0.053855    0.019425   -0.047674    0.032493    0.17906     0.043017\n",
            "  0.054907   -0.029625   -0.11605     0.049639    0.14474     0.037841\n",
            "  0.14143     0.0010015   0.050475    0.052853    0.015726    0.0085179\n",
            " -0.00057323  0.070841   -0.028111    0.029294   -0.031062    0.010723\n",
            "  0.056798   -0.031985    0.049969   -0.021129   -0.034351   -0.047213\n",
            "  0.023057   -0.012029   -0.05057    -0.17281    -0.0068876   0.046897\n",
            " -0.1099     -0.1134      0.0054056  -0.057539    0.058319   -0.05241\n",
            " -0.039214    0.011012   -0.013798    0.031996    0.0034696   0.051652\n",
            " -0.034715    0.0052073   0.076233   -0.049084    0.11352    -0.13398\n",
            "  0.057226    0.075949    0.083673    0.026268   -0.088492    0.040667\n",
            "  0.042617    0.034032    0.1301     -0.071195   -0.10635    -0.083051\n",
            " -0.08793     0.034936    0.02056    -0.092008    0.078329    0.046256\n",
            " -0.060573   -0.0021216  -0.0288     -0.0028923   0.0088775   0.041574\n",
            " -0.1676     -0.056263   -0.1264     -0.039737    0.16309     0.20163\n",
            " -0.10327    -0.0063153  -0.18097    -0.091175    0.068055   -0.014139\n",
            "  0.090772   -0.059165   -0.026555   -0.15513     0.041404    0.057644\n",
            "  0.03796    -0.017287    0.062533    0.15277     0.017118    0.012293\n",
            "  0.039745   -0.0010813  -0.12311    -0.076416    0.056308    0.016645\n",
            " -0.012522   -0.04741    -0.042837    0.043384    0.071481    0.013729\n",
            "  0.28128    -0.020377   -0.086632   -0.1078     -0.071292   -0.0057681\n",
            " -0.051001   -0.0019882  -0.014559   -0.01725     0.044477    0.018904\n",
            "  0.022944    0.048958   -0.044972   -0.057655    0.067673   -0.0011916\n",
            " -0.041793    0.010388    0.14232     0.0144     -0.099152   -0.083794\n",
            "  0.049883    0.0092433   0.065875    0.0064689   0.019854   -0.17432\n",
            "  0.015963   -0.08029    -0.10734     0.010794    0.019445   -0.085087\n",
            " -0.091942    0.021685   -0.059844    0.048794    0.029278   -0.0547\n",
            "  0.092618    0.10009    -0.040408   -0.032016   -0.10172    -0.040733\n",
            " -0.098845    0.11775     0.04845    -0.0065074  -0.076034    0.072858\n",
            "  0.11209     0.020953   -0.020277    0.024254   -0.026014   -0.079301\n",
            "  0.13861     0.24361    -0.041851    0.086242    0.077548    0.017602\n",
            " -0.049762   -0.0059675  -0.01475     0.069983   -0.038368    0.037218\n",
            "  0.028689    0.081729   -0.17901    -0.032387   -0.032801   -0.1232\n",
            "  0.042571   -0.061689    0.12304     0.0063937   0.017244   -0.07078\n",
            "  0.1497     -0.071526   -0.004416   -0.026213    0.16105     0.015106\n",
            "  0.069958    0.06337     0.014418   -0.028337   -0.00657     0.038292\n",
            "  0.059412    0.10985    -0.0025171   0.023026    0.030068    0.1085\n",
            "  0.021813    0.075254    0.020702    0.056185   -0.09952    -0.03743\n",
            "  0.066892   -0.040213   -0.023171   -0.018297    0.051245   -0.030037\n",
            "  0.072421    0.027776    0.075228   -0.053156    0.00065549  0.015825\n",
            " -0.080709    0.10954    -0.095423    0.095556   -0.051174   -0.11047\n",
            " -0.029983   -0.026057   -0.019929   -0.04105    -0.0913      0.041795\n",
            " -0.003382    0.056079    0.033006    0.096085   -0.043703    0.035386\n",
            "  0.045623    0.050434    0.10783    -0.018721    0.046601    0.013269\n",
            " -0.047096    0.096469   -0.049453    0.033148   -0.019341   -0.065587\n",
            " -0.11885    -0.0060759   0.00085925 -0.041288    0.1418      0.10084\n",
            " -0.0517      0.05277    -0.19487     0.013724   -0.057689    0.039048\n",
            "  0.036625    0.10722    -0.08102     0.0072683   0.028318   -0.023709\n",
            " -0.014674    0.062832   -0.0087419   0.086508    0.061904    0.14775   ]\n"
          ]
        }
      ],
      "source": [
        "print(glove['terra'].shape)\n",
        "pl_ptbb=glove['terra']\n",
        "print(pl_ptbb)\n",
        "\n",
        "# print(w2v_v['gente'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CbPGzSLg6PWZ",
      "metadata": {
        "id": "CbPGzSLg6PWZ"
      },
      "source": [
        "Perceba que estamos utilizando modelos pré-treinados com corpus do idioma inglês. Portanto, no corpus ficaram faltando muitas palavras de outros idiomas como o português.\n",
        "\n",
        "Faça o download do arquivo pré-treinado em Word2Vec do FastText no endereço https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.pt.vec e carregue-o em uma variável.\n",
        "\n",
        "**Sugestão:** Faça o download com o comando wget e utilize o método load_word2vec_format() do KeyedVectors da lib gensim.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3ohwUJg0XzB",
      "metadata": {
        "id": "c3ohwUJg0XzB"
      },
      "outputs": [],
      "source": [
        "# Faça o download do arquivo\n",
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.pt.vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84K8Vt1dGOW0",
      "metadata": {
        "id": "84K8Vt1dGOW0"
      },
      "outputs": [],
      "source": [
        "# Carregue o modelo a partir do arquivo baixado\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XUrvljrqLyBh",
      "metadata": {
        "id": "XUrvljrqLyBh"
      },
      "source": [
        "## Escolha suas palavras agora em português e salve em um vetor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V4UfcNYJGHMZ",
      "metadata": {
        "id": "V4UfcNYJGHMZ"
      },
      "outputs": [],
      "source": [
        "palavras = [\"rei\", \"princesa\", \"monarca\", \"trono\", \"coroa\",\n",
        "         \"montanha\", \"oceano\", \"tv\", \"chuva\", \"nuvem\", \"rainha\", \"caderno\", \"escola\", \"recreio\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0msepkrPDnIX",
      "metadata": {
        "id": "0msepkrPDnIX"
      },
      "source": [
        "# **EXERCÍCIO**\n",
        "# **b)** Crie um novo vetor que receberá os embeddings correspondentes às palavras em português que você escolheu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdy9ZAUYCkI3",
      "metadata": {
        "id": "fdy9ZAUYCkI3"
      },
      "outputs": [],
      "source": [
        "embeddings_pt ="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kUyHMDDCNT0U",
      "metadata": {
        "id": "kUyHMDDCNT0U"
      },
      "source": [
        "# **EXERCÍCIO**\n",
        "# **c)** Utilize algum método de redução de dimensionalidade para reduzir os vetores dos embeddings para apenas 2 dimensões.\n",
        "\n",
        "**Sugestão:** Utilize o PCA do sklearn passando como parâmetro n_components=2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf89b540-550a-47cb-a4a5-502d6544d949",
      "metadata": {
        "id": "bf89b540-550a-47cb-a4a5-502d6544d949"
      },
      "outputs": [],
      "source": [
        "# Importando libs necessárias\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Reduz para 2 dimensões usando método PCA\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7TZ_90YZTI0G",
      "metadata": {
        "id": "7TZ_90YZTI0G"
      },
      "source": [
        "# **EXERCÍCIO**\n",
        "# **d)** Faça o \"plot\" em um plano bidimensional com os valores obtidos, se possível, comparando os resultados referentes a diferentes modelos.\n",
        "\n",
        "**Sugestão:** Utilize o código da célula abaixo como exemplo de como efetuar o plot, sendo necessário ajustar a variável que contém os vetores PCA, bem como os rótulos com as palavras selecionadas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "359e1719-dd9b-4130-93f1-8cd2cf97e2d6",
      "metadata": {
        "id": "359e1719-dd9b-4130-93f1-8cd2cf97e2d6"
      },
      "outputs": [],
      "source": [
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, axes = plt.subplots(1, 1, figsize=(5, 5))\n",
        "axes.scatter(vectors_pca[:, 0], vectors_pca[:, 1])\n",
        "for i, p in enumerate(palavras):\n",
        "    axes.annotate(p, (vectors_pca[i, 0]+.02, vectors_pca[i, 1]+.02))\n",
        "axes.set_title('PCA Word Embeddings')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lrNHReUr99R7",
      "metadata": {
        "id": "lrNHReUr99R7"
      },
      "source": [
        "# Agora que você já sabe, faça um teste com outras palavras e veja como fica o plot do gráfico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HRxguhEy9-L5",
      "metadata": {
        "id": "HRxguhEy9-L5"
      },
      "outputs": [],
      "source": [
        "# Inicie aqui definindo suas variáveis\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9pWy_5FVAWE9",
      "metadata": {
        "id": "9pWy_5FVAWE9"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8f6541fd-4b2c-40f3-b52b-feb052f8154b",
      "metadata": {
        "id": "8f6541fd-4b2c-40f3-b52b-feb052f8154b"
      },
      "source": [
        "# Word2Vec algebra\n",
        "# **EXERCÍCIO**\n",
        "# **e)** Execute operações de soma e subtração com os vetores e observe os resultados obtidos.\n",
        "\n",
        "**Sugestão:** Você pode utilizar o método \"most_similar()\" do modelo, utilizando os parâmetros \"positive\" e \"negative\" para operações de soma e subtração. O parâmetro \"topn\" controla a quantidade de resultados.\n",
        "\n",
        "**Observação:** Caso prefira utilizar numpy para as operações, ao chamar o método \"most_similar()\" utilize o parâmetro \"vector\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2fdb948-7ee0-46c6-8423-1bb6c1160bf7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2fdb948-7ee0-46c6-8423-1bb6c1160bf7",
        "outputId": "7bd50e4a-96a1-460f-ff09-c317b1b30c7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    A palavra mais próxima da operação rei - mulher + homem é: 'rainha'\n",
            "    com similaridade 0.6602969765663147\n"
          ]
        }
      ],
      "source": [
        "# Defina as palavras\n",
        "palavra1 = ''\n",
        "palavra2 = ''\n",
        "palavra3 = ''\n",
        "\n",
        "# Efetue a operação algébrica\n",
        "\n",
        "\n",
        "# Exiba o resultado da busca por similaridade\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TmIJjnrABP40",
      "metadata": {
        "id": "TmIJjnrABP40"
      },
      "source": [
        "## Espaço para testar mais operações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fGbaDAm-BI2N",
      "metadata": {
        "id": "fGbaDAm-BI2N"
      },
      "outputs": [],
      "source": [
        "# Defina as palavras\n",
        "\n",
        "# Efetue a operação algébrica\n",
        "\n",
        "# Exiba o resultado da busca por similaridade"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca418d70-69ae-479c-b174-f61a69a834ba",
      "metadata": {
        "id": "ca418d70-69ae-479c-b174-f61a69a834ba"
      },
      "source": [
        "# Word2Vec vs BERT: Embeddings em contextos diferentes\n",
        "\n",
        "Nos modelos representativos mais complexos, como o BERT, o cáculo dos valores de embedding de uma palavra pode depender dramáticamente da aplicação na frase. Isso não é por acaso e faz total sentido, pois a semântica pode ser totalmente diferente.\n",
        "\n",
        "Por exemplo, observe a plavra 'manga' nas duas senteças abaixo:\n",
        "\n",
        "\n",
        "*   \"Sujei a manga da minha camisa.\"\n",
        "*   \"Quero comer manga com leite.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o5WYol4iBTY2",
      "metadata": {
        "id": "o5WYol4iBTY2"
      },
      "source": [
        "## Vamos testar o quanto o modelo consegue capturar o contexto na tokenização incluindo agora o BERT.\n",
        "\n",
        "# **EXERCÍCIO**\n",
        "# **f)** Para comparar outras formas de gerar embeddings, utilize o modelo BERT de forma a gerar word embeddings a partir de frases e veja o resultado obtido.\n",
        "\n",
        "O código a seguir vai auxiliar na instanciação do modelo BERT e definição de uma função para ter acesso facilitado aos embeddings do modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e02a644c-9413-407a-aa6b-d5a5079dbb76",
      "metadata": {
        "id": "e02a644c-9413-407a-aa6b-d5a5079dbb76"
      },
      "outputs": [],
      "source": [
        "#\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Função para obter os embeddings do BERT\n",
        "def get_bert_embeddings(sentence, word):\n",
        "    inputs = tokenizer(sentence, return_tensors='pt')\n",
        "    outputs = model(**inputs)\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "    word_tokens = tokenizer.tokenize(sentence)\n",
        "    word_index = word_tokens.index(word)\n",
        "    word_embedding = last_hidden_states[0, word_index + 1, :]\n",
        "    return word_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uynd92ZhCf0p",
      "metadata": {
        "id": "uynd92ZhCf0p"
      },
      "source": [
        "# **f)** (Detalhamento) Defina duas sentenças que contenham uma mesma palavra em ambas, porém com semântica diferente. Em seguide calcule os embeddings da mesma palavra nas duas sentenças, para finalmente comparar os resultados.\n",
        "\n",
        "**Dica:** Utilize o método definido anteriormente, passando a sentença e a palavra que você deseja obter o embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ASaCTNgabCGh",
      "metadata": {
        "id": "ASaCTNgabCGh"
      },
      "outputs": [],
      "source": [
        "# Defina suas sentenças\n",
        "sentenca1 = \"\"\n",
        "sentenca2 = \"\"\n",
        "\n",
        "# Defina a palavra foco\n",
        "\n",
        "\n",
        "# Calcule os embeddings da palavra nas duas situações\n",
        "\n",
        "\n",
        "# Imprima os embeddings calculados\n",
        "\n",
        "\n",
        "# Em adicional, calcule a similaridade\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}